{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ijson\n",
        "import torch\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3min to process\n",
        "pure_evidence_data = pd.read_json(\"data/evidence.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "pure_dev_data_set = pd.read_json(\"data/dev-claims.json\")\n",
        "pure_train_data_set = pd.read_json(\"data/train-claims.json\")\n",
        "pure_test_data_set = pd.read_json(\"data/test-claims-unlabelled.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'claim_id': 'claim-752', 'claim_text': '[South Australia] has the most expensive electricity in the world.', 'evidence_texts': ['[citation needed] South Australia has the highest retail price for electricity in the country.', '\"South Australia has the highest power prices in the world\".'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-375', 'claim_text': 'when 3 per cent of total annual global emissions of carbon dioxide are from humans and Australia prod\\xaduces 1.3 per cent of this 3 per cent, then no amount of emissions reductio\\xadn here will have any effect on global climate.', 'evidence_texts': ['The 2011 UNEP Green Economy report states that \"[a]agricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions.', 'With a market share of 30% and (potentially) clean electricity, heat pumps could reduce global CO 2 emissions by 8% annually.', 'In the modern era, emissions to the atmosphere from volcanoes are approximately 0.645 billion tonnes of CO 2 per year, whereas humans contribute 29 billion tonnes of CO 2 each year.', 'Cumulative anthropogenic (i.e., human-emitted) emissions of CO 2 from fossil fuel use are a major cause of global warming, and give some indication of which countries have contributed most to human-induced climate change.', 'Other countries with fast growing emissions are South Korea, Iran, and Australia (which apart from the oil rich Persian Gulf states, now has the highest percapita emission rate in the world).'], 'label': 'NOT_ENOUGH_INFO'}, {'claim_id': 'claim-1266', 'claim_text': 'This means that the world is now 1C warmer than it was in pre-industrial times', 'evidence_texts': ['Multiple independently produced instrumental datasets confirm that the 2009–2018 decade was 0.93 ± 0.07\\xa0°C warmer than the pre-industrial baseline (1850–1900).', 'The planet is now 0.8\\xa0°C warmer than in pre-industrial times.'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-871', 'claim_text': '“As it happens, Zika may also be a good model of the second worrying effect — disease mutation.', 'evidence_texts': ['Genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual, or can be inherited.', 'These errors, called mutations, can affect the phenotype of an organism, especially if they occur within the protein coding sequence of a gene.', 'This is especially useful studying diseases in adults by allowing expression after a certain period of growth, thus eliminating the deleterious effect of gene expression seen during stages of development in model organisms.', 'In addition, research was needed to determine the mechanism by which Zika produced these effects.', 'Zika also appears to have an equal tropism for cells of the developing eye, leading to high rates of eye abnormalities as well.'], 'label': 'NOT_ENOUGH_INFO'}, {'claim_id': 'claim-2164', 'claim_text': 'Greenland has only lost a tiny fraction of its ice mass', 'evidence_texts': ['If iceberg calving has happened as an average, Greenland lost 294\\xa0Gt of its mass during 2007 (one km3 of ice weighs about 0.9\\xa0Gt).', 'Findings show that Greenland has lost 3.8 trillion tonnes of ice since 1992, enough to raise sea levels by almost 11mm (1.06cm).', '\"Greenland Glaciers Losing Ice Much Faster, Study Says\".', 'Between then and 2010, the mountain lost 80 percent of its ice — two-thirds of which since another scientific expedition in the 1970s.'], 'label': 'REFUTES'}]\n",
            "[{'claim_id': 'claim-1937', 'claim_text': 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'evidence_texts': ['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', 'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', 'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.'], 'label': 'DISPUTED'}, {'claim_id': 'claim-126', 'claim_text': 'El Niño drove record highs in global temperatures suggesting rise may not be down to man-made emissions.', 'evidence_texts': ['While ‘climate change’ can be due to natural forces or human activity, there is now substantial evidence to indicate that human activity – and specifically increased greenhouse gas (GHGs) emissions – is a key factor in the pace and extent of global temperature increases.', 'This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers.'], 'label': 'REFUTES'}, {'claim_id': 'claim-2510', 'claim_text': 'In 1946, PDO switched to a cool phase.', 'evidence_texts': ['There is evidence of reversals in the prevailing polarity (meaning changes in cool surface waters versus warm surface waters within the region) of the oscillation occurring around 1925, 1947, and 1977; the last two reversals corresponded with dramatic shifts in salmon production regimes in the North Pacific Ocean.', '1945/1946: The PDO changed to a \"cool\" phase, the pattern of this regime shift is similar to the 1970s episode with maximum amplitude in the subarctic and subtropical front but with a greater signature near the Japan while the 1970s shift was stronger near the American west coast.'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-2021', 'claim_text': 'Weather Channel co-founder John Coleman provided evidence that convincingly refutes the concept of anthropogenic global warming.', 'evidence_texts': [\"There is no convincing scientific evidence that human release of carbon dioxide, methane, or other greenhouse gases is causing or will, in the foreseeable future, cause catastrophic heating of the Earth's atmosphere and disruption of the Earth's climate.\", 'He has called global warming the \"greatest scam in history\" and made numerous false or misleading claims about climate science.', 'International Council of Academies of Engineering and Technological Sciences (CAETS) in 2007, issued a Statement on Environment and Sustainable Growth: As reported by the Intergovernmental Panel on Climate Change (IPCC), most of the observed global warming since the mid-20th century is very likely due to human-produced emission of greenhouse gases and this warming will continue unabated if present anthropogenic emissions continue or, worse, expand without control.', '\"Climate Scientists Virtually Unanimous Anthropogenic Global Warming Is True\".', 'Scientists Reach 100% Consensus on Anthropogenic Global Warming.'], 'label': 'DISPUTED'}, {'claim_id': 'claim-2449', 'claim_text': '\"January 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators.', 'evidence_texts': ['With average temperature +8.1\\xa0°C (47\\xa0°F).', 'The Iranian / Persian calendar, currently used in Iran and Afghanistan, also has 12 months.', 'All of these events can have wide variations of more than a month from year to year.', 'Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).', 'It has a duration of approximately 354.37 days.'], 'label': 'NOT_ENOUGH_INFO'}]\n",
            "[{'claim_id': 'claim-2967', 'claim_text': 'The contribution of waste heat to the global climate is 0.028 W/m2.', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-979', 'claim_text': '“Warm weather worsened the most recent five-year drought, which included the driest four-year period on record in terms of statewide precipitation.', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-1609', 'claim_text': 'Greenland has only lost a tiny fraction of its ice mass.', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-1020', 'claim_text': '“The global reef crisis does not necessarily mean extinction for coral species.', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-2599', 'claim_text': 'Small amounts of very active substances can cause large effects.', 'evidence_texts': [], 'label': ''}]\n"
          ]
        }
      ],
      "source": [
        "def structurelise_data_set(data_set):\n",
        "    structured_data_set = []\n",
        "    for claim_id, claim_info in data_set.items():\n",
        "        claim_text = claim_info['claim_text']\n",
        "        claim_label = ''\n",
        "        if 'claim_label' in claim_info:\n",
        "            claim_label = claim_info['claim_label']\n",
        "        evidence_ids = []\n",
        "        if 'evidences' in claim_info:\n",
        "            evidence_ids = claim_info['evidences']\n",
        "        evidence_texts = []\n",
        "        for evidence_id in evidence_ids:\n",
        "            evidence_texts.append(pure_evidence_data[evidence_id][0])\n",
        "        structured_data_set.append({\n",
        "            'claim_id' : claim_id,\n",
        "            'claim_text': claim_text,\n",
        "            'evidence_texts': evidence_texts,\n",
        "            'label': claim_label\n",
        "        })\n",
        "    return structured_data_set\n",
        "\n",
        "structured_dev_data_set = structurelise_data_set(pure_dev_data_set)\n",
        "structured_train_data_set = structurelise_data_set(pure_train_data_set)\n",
        "structured_test_data_set = structurelise_data_set(pure_test_data_set)\n",
        "print(structured_dev_data_set[0:5])\n",
        "print(structured_train_data_set[0:5])\n",
        "print(structured_test_data_set[0:5])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'claim_id': 'claim-752', 'claim_text': 'south australia has the most expensive electricity in the world', 'evidence_texts': ['citation needed south australia has the highest retail price for electricity in the country', 'south australia has the highest power prices in the world'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-375', 'claim_text': 'when 3 per cent of total annual global emissions of carbon dioxide are from humans and australia produces 13 per cent of this 3 per cent then no amount of emissions reduction here will have any effect on global climate', 'evidence_texts': ['the 2011 unep green economy report states that aagricultural operations excluding land use changes produce approximately 13 per cent of anthropogenic global ghg emissions', 'with a market share of 30 and potentially clean electricity heat pumps could reduce global co 2 emissions by 8 annually', 'in the modern era emissions to the atmosphere from volcanoes are approximately 0645 billion tonnes of co 2 per year whereas humans contribute 29 billion tonnes of co 2 each year', 'cumulative anthropogenic ie humanemitted emissions of co 2 from fossil fuel use are a major cause of global warming and give some indication of which countries have contributed most to humaninduced climate change', 'other countries with fast growing emissions are south korea iran and australia which apart from the oil rich persian gulf states now has the highest percapita emission rate in the world'], 'label': 'NOT_ENOUGH_INFO'}, {'claim_id': 'claim-1266', 'claim_text': 'this means that the world is now 1c warmer than it was in preindustrial times', 'evidence_texts': ['multiple independently produced instrumental datasets confirm that the 20092018 decade was 093 007 c warmer than the preindustrial baseline 18501900', 'the planet is now 08 c warmer than in preindustrial times'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-871', 'claim_text': 'as it happens zika may also be a good model of the second worrying effect disease mutation', 'evidence_texts': ['genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual or can be inherited', 'these errors called mutations can affect the phenotype of an organism especially if they occur within the protein coding sequence of a gene', 'this is especially useful studying diseases in adults by allowing expression after a certain period of growth thus eliminating the deleterious effect of gene expression seen during stages of development in model organisms', 'in addition research was needed to determine the mechanism by which zika produced these effects', 'zika also appears to have an equal tropism for cells of the developing eye leading to high rates of eye abnormalities as well'], 'label': 'NOT_ENOUGH_INFO'}, {'claim_id': 'claim-2164', 'claim_text': 'greenland has only lost a tiny fraction of its ice mass', 'evidence_texts': ['if iceberg calving has happened as an average greenland lost 294 gt of its mass during 2007 one km3 of ice weighs about 09 gt', 'findings show that greenland has lost 38 trillion tonnes of ice since 1992 enough to raise sea levels by almost 11mm 106cm', 'greenland glaciers losing ice much faster study says', 'between then and 2010 the mountain lost 80 percent of its ice twothirds of which since another scientific expedition in the 1970s'], 'label': 'REFUTES'}]\n",
            "[{'claim_id': 'claim-1937', 'claim_text': 'not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life', 'evidence_texts': ['at very high concentrations 100 times atmospheric concentration or greater carbon dioxide can be toxic to animal life so raising the concentration to 10000 ppm 1 or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse', 'plants can grow as much as 50 percent faster in concentrations of 1000 ppm co 2 when compared with ambient conditions though this assumes no change in climate and no limitation on other nutrients', 'higher carbon dioxide concentrations will favourably affect plant growth and demand for water'], 'label': 'DISPUTED'}, {'claim_id': 'claim-126', 'claim_text': 'el nio drove record highs in global temperatures suggesting rise may not be down to manmade emissions', 'evidence_texts': ['while climate change can be due to natural forces or human activity there is now substantial evidence to indicate that human activity and specifically increased greenhouse gas ghgs emissions is a key factor in the pace and extent of global temperature increases', 'this acceleration is due mostly to humancaused global warming which is driving thermal expansion of seawater and the melting of landbased ice sheets and glaciers'], 'label': 'REFUTES'}, {'claim_id': 'claim-2510', 'claim_text': 'in 1946 pdo switched to a cool phase', 'evidence_texts': ['there is evidence of reversals in the prevailing polarity meaning changes in cool surface waters versus warm surface waters within the region of the oscillation occurring around 1925 1947 and 1977 the last two reversals corresponded with dramatic shifts in salmon production regimes in the north pacific ocean', '19451946 the pdo changed to a cool phase the pattern of this regime shift is similar to the 1970s episode with maximum amplitude in the subarctic and subtropical front but with a greater signature near the japan while the 1970s shift was stronger near the american west coast'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-2021', 'claim_text': 'weather channel cofounder john coleman provided evidence that convincingly refutes the concept of anthropogenic global warming', 'evidence_texts': ['there is no convincing scientific evidence that human release of carbon dioxide methane or other greenhouse gases is causing or will in the foreseeable future cause catastrophic heating of the earths atmosphere and disruption of the earths climate', 'he has called global warming the greatest scam in history and made numerous false or misleading claims about climate science', 'international council of academies of engineering and technological sciences caets in 2007 issued a statement on environment and sustainable growth as reported by the intergovernmental panel on climate change ipcc most of the observed global warming since the mid20th century is very likely due to humanproduced emission of greenhouse gases and this warming will continue unabated if present anthropogenic emissions continue or worse expand without control', 'climate scientists virtually unanimous anthropogenic global warming is true', 'scientists reach 100 consensus on anthropogenic global warming'], 'label': 'DISPUTED'}, {'claim_id': 'claim-2449', 'claim_text': 'january 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators', 'evidence_texts': ['with average temperature 81 c 47 f', 'the iranian persian calendar currently used in iran and afghanistan also has 12 months', 'all of these events can have wide variations of more than a month from year to year', 'its average duration is 365256363004 days 365 d 6 h 9 min 976 s at the epoch j20000 january 1 2000 120000 tt', 'it has a duration of approximately 35437 days'], 'label': 'NOT_ENOUGH_INFO'}]\n",
            "[{'claim_id': 'claim-2967', 'claim_text': 'the contribution of waste heat to the global climate is 0028 wm2', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-979', 'claim_text': 'warm weather worsened the most recent fiveyear drought which included the driest fouryear period on record in terms of statewide precipitation', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-1609', 'claim_text': 'greenland has only lost a tiny fraction of its ice mass', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-1020', 'claim_text': 'the global reef crisis does not necessarily mean extinction for coral species', 'evidence_texts': [], 'label': ''}, {'claim_id': 'claim-2599', 'claim_text': 'small amounts of very active substances can cause large effects', 'evidence_texts': [], 'label': ''}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    if text:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def preprocess_data_set(data_set):\n",
        "    for item in data_set:\n",
        "        item['claim_text'] = preprocess_text(item['claim_text'])\n",
        "        item['evidence_texts'] = [preprocess_text(evidence) for evidence in item['evidence_texts']]\n",
        "    return data_set\n",
        "\n",
        "processed_dev_data_set = preprocess_data_set(structured_dev_data_set)\n",
        "processed_train_data_set = preprocess_data_set(structured_train_data_set)\n",
        "processed_test_data_set = preprocess_data_set(structured_test_data_set)\n",
        "print(processed_dev_data_set[0:5])\n",
        "print(processed_train_data_set[0:5])\n",
        "print(processed_test_data_set[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'evidence_id': 'evidence-0', 'evidence_text': 'John Bennet Lawes, English entrepreneur and agricultural scientist'}, {'evidence_id': 'evidence-1', 'evidence_text': 'Lindberg began his professional career at the age of 16, eventually moving to New York City in 1977.'}, {'evidence_id': 'evidence-2', 'evidence_text': \"``Boston (Ladies of Cambridge)'' by Vampire Weekend\"}, {'evidence_id': 'evidence-3', 'evidence_text': 'Gerald Francis Goyer (born October 20, 1936) was a professional ice hockey player who played 40 games in the National Hockey League.'}, {'evidence_id': 'evidence-4', 'evidence_text': 'He detected abnormalities of oxytocinergic function in schizoaffective mania, post-partum psychosis and how ECT modified oxytocin release.'}]\n",
            "[{'evidence_id': 'evidence-0', 'evidence_text': 'john bennet lawes english entrepreneur and agricultural scientist'}, {'evidence_id': 'evidence-1', 'evidence_text': 'lindberg began his professional career at the age of 16 eventually moving to new york city in 1977'}, {'evidence_id': 'evidence-2', 'evidence_text': 'boston ladies of cambridge by vampire weekend'}, {'evidence_id': 'evidence-3', 'evidence_text': 'gerald francis goyer born october 20 1936 was a professional ice hockey player who played 40 games in the national hockey league'}, {'evidence_id': 'evidence-4', 'evidence_text': 'he detected abnormalities of oxytocinergic function in schizoaffective mania postpartum psychosis and how ect modified oxytocin release'}]\n"
          ]
        }
      ],
      "source": [
        "structured_evidence_data = []\n",
        "for evidence_id, evidence_text  in pure_evidence_data.items():\n",
        "    structured_evidence_data.append({\n",
        "        'evidence_id' : evidence_id,\n",
        "        'evidence_text': evidence_text[0],\n",
        "    })\n",
        "print(structured_evidence_data[0:5])\n",
        "for item in structured_evidence_data:\n",
        "    item['evidence_text'] = preprocess_text(str(item['evidence_text']))\n",
        "print(structured_evidence_data[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\scott\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word,'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word,'n')\n",
        "    return lemma\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    words = text.lower().split()\n",
        "    new_words = []\n",
        "    for w in words:\n",
        "        w = lemmatize(w)\n",
        "        if w not in stopwords:\n",
        "            new_words.append(w)\n",
        "\n",
        "    processed_text = \" \".join(new_words)\n",
        "    return processed_text\n",
        "\n",
        "# map label from string to number\n",
        "label_dict = {'SUPPORTS': 0,\n",
        "    'REFUTES': 1,\n",
        "    'NOT_ENOUGH_INFO': 2,\n",
        "    'DISPUTED': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combind claim and envidence text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     claim_id                                         claim_text  \\\n",
            "0  claim-1937  scientific evidence co2 pollutant higher co2 c...   \n",
            "\n",
            "                                      evidence_texts  label  \n",
            "0  high concentration 100 time atmospheric concen...      3  \n",
            "(1228, 4)\n",
            "(154, 4)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "train_df = pd.DataFrame(processed_train_data_set)\n",
        "dev_df = pd.DataFrame(processed_dev_data_set)\n",
        "\n",
        "# change label from string to number\n",
        "train_df['label'] = train_df['label'].map(label_dict)\n",
        "dev_df['label'] = dev_df['label'].map(label_dict)\n",
        "\n",
        "# combine all envidence texts\n",
        "train_df['evidence_texts'] = train_df['evidence_texts'].apply(lambda texts: \" \".join(texts))\n",
        "dev_df['evidence_texts'] = dev_df['evidence_texts'].apply(lambda texts: \" \".join(texts))\n",
        "\n",
        "# preprocessing\n",
        "train_df['claim_text'] = train_df['claim_text'].apply(text_preprocessing)\n",
        "train_df['evidence_texts'] = train_df['evidence_texts'].apply(text_preprocessing)\n",
        "dev_df['claim_text'] = dev_df['claim_text'].apply(text_preprocessing)\n",
        "dev_df['evidence_texts'] = dev_df['evidence_texts'].apply(text_preprocessing)\n",
        "\n",
        "print(train_df.head(1))\n",
        "print(train_df.shape)\n",
        "print(dev_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resample to balance the dev data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "0    68\n",
            "2    41\n",
            "1    27\n",
            "3    18\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "0    18\n",
            "1    18\n",
            "2    18\n",
            "3    18\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(dev_df['label'].value_counts())\n",
        "# get data count by labels\n",
        "df_SUPPORTS = dev_df[dev_df.label == 0]\n",
        "df_REFUTES = dev_df[dev_df.label == 1]\n",
        "df_NOT_ENOUGH_INFO = dev_df[dev_df.label == 2]\n",
        "df_DISPUTED = dev_df[dev_df.label == 3]\n",
        "\n",
        "# resample\n",
        "df_SUPPORTS_sampled = resample(df_SUPPORTS, replace=True, n_samples=len(df_DISPUTED), random_state=1)\n",
        "df_REFUTES_sampled = resample(df_REFUTES, replace=True, n_samples=len(df_DISPUTED), random_state=2)\n",
        "df_NOT_ENOUGH_INFO_sampled = resample(df_NOT_ENOUGH_INFO, replace=True, n_samples=len(df_DISPUTED), random_state=3)\n",
        "\n",
        "# concat them\n",
        "df_sampled = pd.concat([df_SUPPORTS_sampled, df_REFUTES_sampled, df_NOT_ENOUGH_INFO_sampled, df_DISPUTED])\n",
        "print(df_sampled['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splite data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 2000)\n",
            "(154, 2000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer feature and get top 1000\n",
        "vectorizer1 = TfidfVectorizer(max_features=1000)\n",
        "vectorizer2 = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# splite data into X_train, y_train, X_dev and y_dev\n",
        "X_train_tfidf_claim = pd.DataFrame(vectorizer1.fit_transform(train_df['claim_text']).toarray(), columns=vectorizer1.get_feature_names_out())\n",
        "X_train_tfidf_evidence = pd.DataFrame(vectorizer2.fit_transform(train_df['evidence_texts']).toarray(), columns=vectorizer2.get_feature_names_out())\n",
        "X_train_tfidf = pd.concat([X_train_tfidf_claim, X_train_tfidf_evidence], axis=1)\n",
        "\n",
        "X_dev_tfidf_claim = pd.DataFrame(vectorizer1.transform(dev_df['claim_text']).toarray(), columns=vectorizer1.get_feature_names_out())\n",
        "X_dev_tfidf_evidence = pd.DataFrame(vectorizer2.transform(dev_df['evidence_texts']).toarray(), columns=vectorizer2.get_feature_names_out())\n",
        "X_dev_tfidf = pd.concat([X_dev_tfidf_claim, X_dev_tfidf_evidence], axis=1)\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_dev = dev_df['label']\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_dev_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baseline model: Zero-rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4416\n",
            "F1-score: 0.1532\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf.fit(X_train_tfidf, y_train)\n",
        "y_predict = dummy_clf.predict(X_dev_tfidf)\n",
        "print(f'Accuracy: {accuracy_score(y_dev, y_predict):.4f}')\n",
        "f1 = f1_score(y_dev, y_predict, average='macro')\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1228, 2, 1000])\n",
            "torch.Size([154, 2, 1000])\n"
          ]
        }
      ],
      "source": [
        "X_train_tfidf = X_train_tfidf.values.reshape(1228, 2, 1000)\n",
        "X_train_tfidf = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
        "\n",
        "X_dev_tfidf = X_dev_tfidf.values.reshape(154, 2, 1000)\n",
        "X_dev_tfidf = torch.tensor(X_dev_tfidf, dtype=torch.float32)\n",
        "\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.int64)\n",
        "y_dev = torch.tensor(y_dev.values, dtype=torch.int64)\n",
        "\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_dev_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 1.3925\n",
            "Epoch [2/5], Loss: 1.3062\n",
            "Epoch [3/5], Loss: 1.2390\n",
            "Epoch [4/5], Loss: 1.2089\n",
            "Epoch [5/5], Loss: 1.1904\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "input_size = 1000\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "output_size = 4\n",
        "\n",
        "# \n",
        "model = GRUModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = model(X_train_tfidf)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.91      0.59        68\n",
            "           1       0.00      0.00      0.00        27\n",
            "           2       0.31      0.10      0.15        41\n",
            "           3       0.00      0.00      0.00        18\n",
            "\n",
            "    accuracy                           0.43       154\n",
            "   macro avg       0.19      0.25      0.19       154\n",
            "weighted avg       0.28      0.43      0.30       154\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs = model(X_dev_tfidf)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    predictions = predicted.numpy()\n",
        "    labels = y_dev.numpy()\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "class_report = classification_report(labels, predictions,zero_division=0)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
