{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ijson\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3min to process\n",
        "# 自己定义地址\n",
        "pure_evidence_data = pd.read_json(\"C:/Users/Administrator/Desktop/UNI/Master/NLP/Proj/evidence.json\", lines=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "pure_dev_data_set = pd.read_json(\"data/dev-claims.json\")\n",
        "pure_train_data_set = pd.read_json(\"data/train-claims.json\")\n",
        "pure_test_data_set = pd.read_json(\"data/test-claims-unlabelled.json\")\n",
        "pure_dev_baseline_date_set = pd.read_json(\"data/dev-claims-baseline.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'claim_id': 'claim-1937', 'claim_text': 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'evidence_texts': ['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', 'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', 'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.'], 'label': 'DISPUTED'}, {'claim_id': 'claim-126', 'claim_text': 'El Niño drove record highs in global temperatures suggesting rise may not be down to man-made emissions.', 'evidence_texts': ['While ‘climate change’ can be due to natural forces or human activity, there is now substantial evidence to indicate that human activity – and specifically increased greenhouse gas (GHGs) emissions – is a key factor in the pace and extent of global temperature increases.', 'This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers.'], 'label': 'REFUTES'}, {'claim_id': 'claim-2510', 'claim_text': 'In 1946, PDO switched to a cool phase.', 'evidence_texts': ['There is evidence of reversals in the prevailing polarity (meaning changes in cool surface waters versus warm surface waters within the region) of the oscillation occurring around 1925, 1947, and 1977; the last two reversals corresponded with dramatic shifts in salmon production regimes in the North Pacific Ocean.', '1945/1946: The PDO changed to a \"cool\" phase, the pattern of this regime shift is similar to the 1970s episode with maximum amplitude in the subarctic and subtropical front but with a greater signature near the Japan while the 1970s shift was stronger near the American west coast.'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-2021', 'claim_text': 'Weather Channel co-founder John Coleman provided evidence that convincingly refutes the concept of anthropogenic global warming.', 'evidence_texts': [\"There is no convincing scientific evidence that human release of carbon dioxide, methane, or other greenhouse gases is causing or will, in the foreseeable future, cause catastrophic heating of the Earth's atmosphere and disruption of the Earth's climate.\", 'He has called global warming the \"greatest scam in history\" and made numerous false or misleading claims about climate science.', 'International Council of Academies of Engineering and Technological Sciences (CAETS) in 2007, issued a Statement on Environment and Sustainable Growth: As reported by the Intergovernmental Panel on Climate Change (IPCC), most of the observed global warming since the mid-20th century is very likely due to human-produced emission of greenhouse gases and this warming will continue unabated if present anthropogenic emissions continue or, worse, expand without control.', '\"Climate Scientists Virtually Unanimous Anthropogenic Global Warming Is True\".', 'Scientists Reach 100% Consensus on Anthropogenic Global Warming.'], 'label': 'DISPUTED'}, {'claim_id': 'claim-2449', 'claim_text': '\"January 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators.', 'evidence_texts': ['With average temperature +8.1\\xa0°C (47\\xa0°F).', 'The Iranian / Persian calendar, currently used in Iran and Afghanistan, also has 12 months.', 'All of these events can have wide variations of more than a month from year to year.', 'Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).', 'It has a duration of approximately 354.37 days.'], 'label': 'NOT_ENOUGH_INFO'}]\n"
          ]
        }
      ],
      "source": [
        "def structurelise_data_set(data_set):\n",
        "    structured_data_set = []\n",
        "    for claim_id, claim_info in data_set.items():\n",
        "        claim_text = claim_info['claim_text']\n",
        "        claim_label = ''\n",
        "        if 'claim_label' in claim_info:\n",
        "            claim_label = claim_info['claim_label']\n",
        "        evidence_ids = []\n",
        "        if 'evidences' in claim_info:\n",
        "            evidence_ids = claim_info['evidences']\n",
        "        evidence_texts = []\n",
        "        for evidence_id in evidence_ids:\n",
        "            evidence_texts.append(pure_evidence_data[evidence_id][0])\n",
        "        structured_data_set.append({\n",
        "            'claim_id' : claim_id,\n",
        "            'claim_text': claim_text,\n",
        "            'evidence_texts': evidence_texts,\n",
        "            'label': claim_label\n",
        "        })\n",
        "    return structured_data_set\n",
        "\n",
        "structured_dev_data_set = structurelise_data_set(pure_dev_data_set)\n",
        "structured_train_data_set = structurelise_data_set(pure_train_data_set)\n",
        "structured_test_data_set = structurelise_data_set(pure_test_data_set)\n",
        "structured_dev_baseline_date_set = structurelise_data_set(pure_dev_baseline_date_set)\n",
        "print(structured_train_data_set[0:5])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'claim_id': 'claim-1937', 'claim_text': 'scientific evidence co2 pollutant higher co2 concentration actually help ecosystem support plant animal life', 'evidence_texts': ['high concentration 100 time atmospheric concentration greater carbon dioxide toxic animal life raise concentration 10000 ppm 1 higher several hour eliminate pest whitefly spider mite greenhouse', 'plant grow much 50 percent faster concentration 1000 ppm co 2 compare ambient condition though assume change climate limitation nutrient', 'higher carbon dioxide concentration favourably affect plant growth demand water'], 'label': 'DISPUTED'}, {'claim_id': 'claim-126', 'claim_text': 'el nio drive record high global temperature suggest rise may manmade emission', 'evidence_texts': ['climate change due natural force human activity substantial evidence indicate human activity specifically increase greenhouse gas ghgs emission key factor pace extent global temperature increase', 'acceleration due mostly humancaused global warm drive thermal expansion seawater melt landbased ice sheet glacier'], 'label': 'REFUTES'}, {'claim_id': 'claim-2510', 'claim_text': '1946 pdo switch cool phase', 'evidence_texts': ['evidence reversal prevail polarity mean change cool surface water versus warm surface water within region oscillation occur around 1925 1947 1977 last two reversal correspond dramatic shift salmon production regime north pacific ocean', '19451946 pdo change cool phase pattern regime shift similar 1970s episode maximum amplitude subarctic subtropical front greater signature near japan 1970s shift stronger near american west coast'], 'label': 'SUPPORTS'}, {'claim_id': 'claim-2021', 'claim_text': 'weather channel cofounder john coleman provide evidence convincingly refute concept anthropogenic global warm', 'evidence_texts': ['convince scientific evidence human release carbon dioxide methane greenhouse gas cause foreseeable future cause catastrophic heat earth atmosphere disruption earth climate', 'call global warm greatest scam history make numerous false mislead claim climate science', 'international council academy engineer technological science caets 2007 issue statement environment sustainable growth report intergovernmental panel climate change ipcc observe global warm since mid20th century likely due humanproduced emission greenhouse gas warm continue unabated present anthropogenic emission continue worse expand without control', 'climate scientist virtually unanimous anthropogenic global warm true', 'scientist reach 100 consensus anthropogenic global warm'], 'label': 'DISPUTED'}, {'claim_id': 'claim-2449', 'claim_text': 'january 2008 cap 12 month period global temperature drop major well respect indicator', 'evidence_texts': ['average temperature 81 c 47 f', 'iranian persian calendar currently use iran afghanistan also 12 month', 'event wide variation month year year', 'average duration 365256363004 day 365 6 h 9 min 976 epoch j20000 january 1 2000 120000 tt', 'duration approximately 35437 day'], 'label': 'NOT_ENOUGH_INFO'}]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word,'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word,'n')\n",
        "    return lemma\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    words = text.lower().split()  \n",
        "    \n",
        "    new_words = []\n",
        "    for w in words:\n",
        "        w = lemmatize(w)\n",
        "        if w not in stopwords:\n",
        "            new_words.append(w)\n",
        "\n",
        "    processed_text = \" \".join(new_words)\n",
        "    return processed_text    \n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        word = lemmatize(word)\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)\n",
        "    processed_text = \" \".join(new_words)\n",
        "    return processed_text\n",
        "\n",
        "def preprocess_data_set(data_set):\n",
        "    for item in data_set:\n",
        "        item['claim_text'] = preprocess_text(item['claim_text'])\n",
        "        item['evidence_texts'] = [preprocess_text(evidence) for evidence in item['evidence_texts']]\n",
        "    return data_set\n",
        "\n",
        "processed_dev_data_set = preprocess_data_set(structured_dev_data_set)\n",
        "processed_train_data_set = preprocess_data_set(structured_train_data_set)\n",
        "processed_test_data_set = preprocess_data_set(structured_test_data_set)\n",
        "processed_dev_baseline_date_set = preprocess_data_set(structured_dev_baseline_date_set)\n",
        "print(processed_train_data_set[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "structured_evidence_data = []\n",
        "for evidence_id, evidence_text  in pure_evidence_data.items():\n",
        "    structured_evidence_data.append({\n",
        "        'evidence_id' : evidence_id,\n",
        "        'evidence_text': evidence_text[0],\n",
        "    })\n",
        "for item in structured_evidence_data:\n",
        "    item['evidence_text'] = preprocess_text(str(item['evidence_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n",
            "{'evidence_id': 'evidence-1208826', 'evidence_text': 'storyline revolve around giant plesiosaur akin loch ness monster appear crater lake northern california near susanville confuse much famous crater lake oregon'}\n",
            "1228\n",
            "154\n"
          ]
        }
      ],
      "source": [
        "print(len(structured_evidence_data))\n",
        "print(structured_evidence_data[1208826])\n",
        "print(len(structured_train_data_set))\n",
        "print(len(structured_dev_data_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "355\n",
            "58 year political life arrest 17 time participate gandhiji satyagraha kisan mazdoor harijan student movement actually spend total 13 year various jail different occasion sentence 10 year rigorous imprisonment gorakhpur conspiracy case august 1942 movement keep condemn cell 26 month 1942 44 shoot august rebellion august 6 1942 receive bullet injury nepal freedom struggle december 21 1950 president samajwadi congress party uttar pradesh ii tax satyagraha compaign committee iii kanpur district congress committee 1930 33 iv gorakhpur district congress committee 1937 52 v kisan mazdoor praja party 1952 60 member uttar pradesh congress committee india congress committee 1928 51 ii executive uttar pradesh pradesh congress committee 1937 52 iii uttar pradesh bihar sugar control board many year since 1937 iv sugar labour inquiry committee khaitan committee 1938 40 v executive council sanskrit university varanasi president allindia sugarcane grower association ii unite chini mill mazdoor federation expresident allindia sugar distillery worker federation ii allindia food tobacco hotel worker federation iii allindia transport worker federation iv allindia dock port worker federation v allindia federation government employee union new delhi vi northeastern railwayman union 1937 48 vii uttar pradesh kisan sabha elect vicepresident allindia trade union congress 1947 hind mazdoor sabha 1948 member presidium unite trade union congress 1950 establish ganesh shankar vidyarthi smarak inter college maharajganj 1940 ii jawaharlal nehru smarak post graduate college 1966 maharajganj iii lal bahadur shastri smarak degree college anand nagar 1973 iv choteylal damodar prasad shibbanlal degree college bisokdar sirwa bazar gorakhpur district uttar pradesh act president manage committee aforesaid college member uttar pradesh legislative assembly 1937 46 1964 67 ii constituent assembly india 1946 50 iii provisional parliament 1950 52 iv first lok sabha 1954 57 v second lok sabha 1957 62 vi fifth lok sabha 1971 77 several time member estimate committee ii public account committee iii committee public undertake represent indian labour numerous international labour conference various foreign country connection otherwise visit several country represent sugar worker federation international conference moscow ussr 1954 sofia bulgaria 1957 budapest hungary 1960 prague czechoslovakia 1965 represent transport worker international conference bucharest romania 1956 attend world peace council meet berlin 1954 stockholm 1955\n"
          ]
        }
      ],
      "source": [
        "longest_sentence = 0\n",
        "longest_sentence_text = \"\"\n",
        "for item in structured_evidence_data:\n",
        "    if len(item['evidence_text'].split())> longest_sentence:\n",
        "        longest_sentence = len(item['evidence_text'].split())\n",
        "        longest_sentence_text = item['evidence_text']\n",
        "print(longest_sentence)\n",
        "print(longest_sentence_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further data process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# global word count\n",
        "word_count = Counter()\n",
        "for claim in processed_train_data_set:\n",
        "    word_count.update(tokenize(claim['claim_text']))\n",
        "for evidence in structured_evidence_data:\n",
        "    word_count.update(tokenize(evidence['evidence_text']))\n",
        "\n",
        "sorted_word_count = {word: count for word, count in sorted(word_count.items(), key=lambda x : x[1], reverse=True)}\n",
        "\n",
        "# create vocab with index\n",
        "vocabulary = {'<PAD>' : 0, '<CLS>': 1, '<UNK>' : 2, '<SEP>': 3}\n",
        "vocabulary.update({word: idx + 4 for idx, word in enumerate(sorted_word_count.keys())})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('also', 66990), ('state', 57325), ('bear', 54596), ('first', 50702), ('one', 47006), ('new', 43508), ('include', 39604), ('play', 38769), ('use', 38202), ('year', 38197)]\n",
            "[('<PAD>', 0), ('<CLS>', 1), ('<UNK>', 2), ('<SEP>', 3), ('also', 4), ('state', 5), ('bear', 6), ('first', 7), ('one', 8), ('new', 9)]\n"
          ]
        }
      ],
      "source": [
        "print(list(sorted_word_count.items())[0:10])\n",
        "print(list(vocabulary.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "644056\n"
          ]
        }
      ],
      "source": [
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "label2index = {'SUPPORTS':0, 'NOT_ENOUGH_INFO':1, 'REFUTES':2, 'DISPUTED':3}\n",
        "index2label = {0:'SUPPORTS', 1:'NOT_ENOUGH_INFO', 2:'REFUTES', 3:'DISPUTED'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_sequence(text, vocab, max_length):\n",
        "    tokens = tokenize(text)\n",
        "    token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens if token in vocab]\n",
        "\n",
        "    if (len(token_ids) > max_length):\n",
        "        token_ids = token_ids[:max_length]\n",
        "    else :\n",
        "        token_ids += [vocabulary['<PAD>']] * (max_length - len(token_ids))\n",
        "    return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ClaimCheckDataset(Dataset):\n",
        "    def __init__(self, data, vocabulary, max_length):\n",
        "        self.data = data\n",
        "        self.vocabulary = vocabulary\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        claim_text = text_to_sequence(item[\"claim_text\"], self.vocabulary, self.max_length)\n",
        "        evidence_text = ' <SEP> '.join(item['evidence_texts'])\n",
        "        evidence_text = text_to_sequence(evidence_text, self.vocabulary, self.max_length)\n",
        "        label = label2index[item['label']]\n",
        "        return {\n",
        "            'claim_text': torch.tensor(claim_text, dtype=torch.long),\n",
        "            'evidence_text': torch.tensor(evidence_text, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "train_dataset = ClaimCheckDataset(processed_train_data_set, vocabulary, max_length=512)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate_dataset = ClaimCheckDataset(processed_dev_data_set, vocabulary, max_length=512)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_vocab, num_dimension, num_head, num_hidden, num_layer, num_category, max_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.num_hidden = num_hidden\n",
        "        # transfrom word into vector through embed\n",
        "        self.embed = nn.Embedding(num_vocab, num_dimension)\n",
        "        # transfrom sequence into vector\n",
        "        self.pos_embed = nn.Embedding(max_length, num_dimension)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=num_dimension, nhead=num_head, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layer, norm=nn.LayerNorm(num_hidden))\n",
        "        self.hidden_layer = nn.Linear(num_hidden, num_hidden // 2)\n",
        "        self.classifier = nn.Linear(num_hidden // 2, num_category)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        mask = text == 0\n",
        "        batch_size, seq_length = text.size()\n",
        "        position = torch.arange(seq_length, device=text.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        text_x = self.embed(text) + self.pos_embed(position)\n",
        "        x_encoded = self.transformer_encoder(text_x, src_key_padding_mask = mask)\n",
        "        x_pooled = x_encoded.mean(dim=1)\n",
        "        layer_x = torch.tanh(self.hidden_layer(x_pooled))\n",
        "        label_x = self.classifier(layer_x)\n",
        "        return label_x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer_model = TransformerModel(num_vocab=len(vocabulary), num_dimension=124, num_head=4, num_hidden=124, num_layer=3, num_category = 4, max_length=512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3518,  0.1872, -0.2252, -0.0151]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([1088, 28287, 2377, 2004, 1748]).unsqueeze(0)\n",
        "output = transformer_model(inputs)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.3223, 0.2734, 0.1810, 0.2233]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "probabilities = torch.softmax(output, dim=1)\n",
        "print(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0])\n"
          ]
        }
      ],
      "source": [
        "predicted_class = torch.argmax(probabilities, dim=1)\n",
        "print(predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.54614268663602\n",
            "Epoch 2, Loss: 2.541097864126548\n",
            "Epoch 3, Loss: 2.533978880980076\n",
            "Epoch 4, Loss: 2.533428213535211\n",
            "Epoch 5, Loss: 2.533907801677019\n"
          ]
        }
      ],
      "source": [
        "#Train\n",
        "\n",
        "# 定义损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义优化器\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # 设置模型为训练模式\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            claims, evidences, labels = batch['claim_text'], batch['evidence_text'], batch['label']\n",
        "            # 清空优化器\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # 前向传播\n",
        "            outputs = model(claims)  # 确保你的模型和数据加载方式是匹配的\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # 反向传播\n",
        "            loss.backward()\n",
        "            \n",
        "            # 参数更新\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 前向传播\n",
        "            outputs = model(evidences)  # 确保你的模型和数据加载方式是匹配的\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # 反向传播\n",
        "            loss.backward()\n",
        "            \n",
        "            # 参数更新\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        # 打印每个epoch的损失\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "train_model(transformer_model, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 1.365264630317688\n",
            "Validation F1 Score: 0.10228011329755515\n",
            "Validation Accuracy: 0.4451923370361328\n"
          ]
        }
      ],
      "source": [
        "#Eval\n",
        "from sklearn.metrics import f1_score\n",
        "def calculate_accuracy(y_pred, y_true):\n",
        "    prediction = torch.argmax(y_pred, dim=1)\n",
        "    correct = (prediction == y_true).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "def calculate_f1(y_pred, y_true, average = \"macro\"):\n",
        "    # 将预测结果转换为类别索引\n",
        "    y_pred = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    # 使用 sklearn 的 f1_score 函数计算 F1 分数\n",
        "    return f1_score(y_true, y_pred, average=average)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()  # 设置模型为评估模式\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_f1 = 0\n",
        "    num_batch = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            claims, evidences, labels = batch['claim_text'], batch['evidence_text'], batch['label']\n",
        "            outputs = model(claims)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += calculate_accuracy(output, labels)\n",
        "            total_f1 += calculate_f1(outputs, labels)\n",
        "            num_batch += 1\n",
        "    \n",
        "    print(f'Validation Loss: {total_loss/len(valid_loader)}')\n",
        "    print(f'Validation F1 Score: {total_f1 / num_batch}')\n",
        "    print(f'Validation Accuracy: {total_accuracy / len(valid_loader)}')\n",
        "evaluate_model(transformer_model, validate_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
